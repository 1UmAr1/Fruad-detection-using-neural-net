
library(data.table)
library(pROC)
library(xgboost)
# reading the csv file


fruad1 <- read.csv(file = file.choose(), header = T)
#converting the file to a data frame

fruad <- as.data.frame(fruad1)
summary(fruad)
#checking the column names

colnames(fruad)
# checking the varible amount and class among 31 variables of dataframe

fruad$Amount
fruad$Class

tail(fruad$Amount)

table(fruad$Amount)
which(is.na(fruad))

#looks like we have got no missing values

summary(fruad$Class)
# here mean is too low which means we have got a small number of fruad
# subsetting the data to reduce the computation
set.seed(111)

frac <- data.frame(fruad[1:30000, c(1:31)])
frac2 <- data.frame(fruad[90000:12000, c(1:31)])

# ploting histogtams of variables
hist(frac$V1)
hist(frac$Time)
hist(frac$Amount)
hist(frac$Class)
hist(frac$V2)
hist(frac$V3)
hist(frac$V4)
hist(frac$V5)
hist(frac$V6)
hist(frac$V7)
hist(frac$V8)
hist(frac$V9)
hist(frac$V10)
hist(frac$V11)
hist(frac$V12)
hist(frac$V13)
hist(frac$V14)
hist(frac$V15)
hist(frac$V16)
hist(frac$V17)
hist(frac$V18)
hist(frac$V19)
hist(frac$V20)
hist(frac$V21)
hist(frac$V22)
hist(frac$V23)
hist(frac$V24)
hist(frac$V25)
# most of the variables i.e mostly transactions are clustered around 0
head(fruad)
library(ggplot2)
#this plot shows that the data is unbalanced as we don't have
#many fruads
ggplot(fruad, aes(x = Class)) +
  geom_bar() +
  ggtitle("Number of class labels")
#checking how usefull the amount variable is
ggplot(fruad, aes(x = fruad$Class, y = fruad$Amount)) +
  geom_boxplot()+
  ggtitle("Distribution of transaction amount by class variable")


# well its not working
#compute the mean and median for each class
fruad %>% group_by(Class) %>% summarise(mean(Amount), median(Amount))
#the fruad cases have a heigher mean value(122)
fruad$Class <- as.numeric(fruad$Class)
# plotting a correlation plot for the variables
library(corrplot)
corrplot(cor(fruad[,-c(1)]), method = "square", type = "full", bg = "steelblue2")

#check the fruad Class variable to determine fruads
invalid <- fruad[which(fruad$Class == 1), ]
invalid1 <- frac[which(frac$Class == 1), ]
invalid2 <- frac2[which(frac2$Class == 1), ]
#this shows that the the varience is -ive i.e Time & class move
# in opposite direction


cor(fruad$Time, fruad$Class)
# similarly checking the correlation of other variables
#the correlation is positive which means that their is some relation between

#class and amount
cor(fruad$Class, fruad$Amount)
cor(fruad$V2, fruad$Class)
cor(fruad$V11, fruad$Class)
cor(fruad$V18, fruad$Class)
#applying transformation to the Amount variable
normalize <- function(x){
  return ((x - mean(x, na.rm = T))/sd(x, na.rm = T))
}
fruad$Amount <- normalize(fruad$Amount)
fruad$Class
str(fruad$Class)
table(fruad$Class)
summary(fruad$Class)
train <- fruad$Class
train2 <- data.frame(fruad$Class)
length(unique(fruad$Class))
ggplot(frac, aes(x = frac$V1, fill = frac$Class)) +
  geom_bar() +
  xlab("V1") +
  ylab("fruad") +
  labs(fill = "Fruads")


ggplot(frac, aes(x = Class, y = V1)) +
  geom_boxplot() +
  xlab("V1") +
  ylab("fruad") +
  labs(fill = "Fruads")


ggplot(frac, aes(x = frac$V3, fill = frac$Class)) +
  geom_bar() +
  xlab("V1") +
  ylab("fruad") +
  labs(fill = "Fruads")


ggplot(frac, aes(x = frac$Amount, fill = frac$Class)) +
  geom_bar() +
  xlab("V1") +
  ylab("fruad") +
  labs(fill = "Fruads")

library(caret)
library(Rtsne)

#reducing dimensionality using t-distributed Stochastic Neighbour Embedding
#this shows if their is a pattern in the data which the model could learn
#if pattern dosn't exist we use feature enggineering to try to find one

set <-1:as.integer((0.1* nrow(fruad)))
#removing variable time and class
tss <- fruad[set, (-1)]
tsss <- tss[set, (-30)]


#not
class <- as.factor(fruad$Class[set])

ts <- as.data.frame(ts1$Y)



set.seed(1345)
fruad$Class <- as.numeric(fruad$Class)

# creating 2 data sets one for training and one for testing
traind <- createDataPartition(fruad$Class, times = 1, p = 0.8, list = F)

# the test data set should be partioned from the train data set
# if these two are not partioned we might train some of the data
# that we want our algo to test on i.e test data set giving
#us high prediction but will be mostly because overfitting
trainf <- fruad[traind]
testf <- fruad[!traind]
traing <- fruad$Class[traind]
testg <- fruad$Class[-traind]



library(dplyr)
library(brms)
library(glmnet)


#using random forest
trainrf <- trainf
trainrf$Class <- as.factor(trainrf$Class)





md <- glm(formula = Class ~ fruad., family = "binomial",
          data = trainf)

library(xgboost)

#gradient boosting 
d <- xgb.DMatrix(data = as.matrix(trainf[1, ()]), label = as.numeric(trainf$Class))
d <- xgb.DMatrix(data = as.matrix(trainf[, -c(31)]), label = as.numeric(trainf$Class))
d <- xgb.DMatrix(data = as.matrix(trainf[, -c("Class")]), label = as.numeric(trainf$Class))


normalize <- function(x){
  return ((x - mean(x, na.rm = T))/sd(x, na.rm = T))
}

fruad$Amount <- normalize(fruad$Amount)
trainf$Class

class <- as.factor(fruad$Class[set])
set <- 1:as.integer((0.1* nrow(fruad)))


#removing variable time and class

tss <- fruad[set, (-1)]
tsss <- tss[set, (-30)]

ts1 <-Rtsne(tsss, perplexity = 40, theta = 0.5,
            pcs = F, verboss = T, max_iter = 700, check_duplicates = F)
#not



#setting preplexity = 40
tsne <- Rtsne(fruad[set,-c(1, 31)], perplexity = 40, theta = 0.5,
              pca = F, verbose = T, max_iter = 500, check_duplicates = F)

class <- as.factor(fruad$Class[set])
mt <- as.data.frame(tsne$Y)

#visualising transaction of tsne
ggplot(mt, aes(x = V1, y = V2)) +
  geom_point(aes(color = class)) +
  theme_minimal() +
  ggtitle("Transaction VIsualiasation")

#here we are creating training and testing data.
itrain <- createDataPartition(fruad$Class, times = 1, p = 0.8, list = F)
fruad$Class <- as.numeric(fruad$Class)
train1 <- fruad[itrain]
test1 <- fruad[!itrain]
train2 <- fruad[itrain]
test2 <- fruad[-itrain]




library(doSNOW)

# have to register < R does not have auto register for dosnow
cl <- makeCluster(8, type = "SOCK")
registerDoSNOW(cl)


# have to register < R does not have auto register for dosnow
#using 20 fold cross validation
registerDoSNOW(cl)
ct <- trainControl(method = "cv",
                   number = 20,
                   verboseIter = T,
                   classProbs = T,
                   sampling = "smote",
                   summaryFunction = twoClassSummary,
                   savePredictions = T)




train1_rf <- train1
train1_rf$Class <- as.factor(train1_rf$Class)

fruad$class <- as.numeric(fruad$Class)

f <- as.formula(paste("Class ~ ", paste(fruad[!fruad %in% "Class"])))
f2 <- reformulate(setdiff(colnames(fruad), "Class"), response = "Class")



md0 <- glm(formula = f2, family = "binomial", data = train2)
md1 <- glm(formula = f, family = "binomial", data = train1)
md3 <- glm(formula = Class ~ ., famly = "binomial", data = train1)

#using threshhold 0.5 to change prediction in binary
#increase threshhold to adjust 

mat <- confusionMatrix(test2, as.numeric(predict(md, test1, type = "response") > 0.5))
mat


library(neuralnet)

md1 <- neuralnet(formula = Class ~ ., fruad, hidden = 10, threshold = 0.05)
md1
mat2 <- confusionMatrix(test2, as.numeric(predict(md1, test1, type = "response") > 0.5)) 

plot(md1)

library(randomForest)
train1

levels(train1) <- make.names(c(0, 1))
model_rf_smote <- train(f, data = train1, method = "rf", trControl = ctrl, verbose = T, metric = "ROC")